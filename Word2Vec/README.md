# Word2Vec Skip-gram do Zero com PyTorch

Este reposit√≥rio cont√©m uma implementa√ß√£o educacional do algoritmo **Word2Vec**, especificamente a arquitetura **Skip-gram**, desenvolvida do zero utilizando a biblioteca **PyTorch**.

O objetivo deste projeto √© demonstrar o funcionamento interno das representa√ß√µes vetoriais de palavras (embeddings) e como elas capturam rela√ß√µes sem√¢nticas entre termos em um corpus de texto.

## üöÄ Vis√£o Geral

O modelo foi treinado em um pequeno corpus de exemplo contendo frases sobre frutas e animais para ilustrar como o Word2Vec agrupa conceitos semelhantes no espa√ßo vetorial.

### Principais Etapas do C√≥digo:

1. **Pr√©-processamento de Dados:** Tokeniza√ß√£o e cria√ß√£o de dicion√°rios `word2index` e `index2word`.
2. **Prepara√ß√£o de Dados (Skip-gram):** Gera√ß√£o de pares (palavra_central, palavra_contexto).
3. **Arquitetura do Modelo:** Implementa√ß√£o da classe `Skipgram` com camadas de `nn.Embedding`.
4. **Treinamento:** Loop de treinamento manual utilizando o otimizador Adam e c√°lculo de Log-Likelihood.
5. **Visualiza√ß√£o:** Plotagem dos embeddings em 2D usando Matplotlib para verificar a proximidade sem√¢ntica (ex: "apple" perto de "fruit").

## üõ†Ô∏è Tecnologias Utilizadas

* **Python 3**
* **PyTorch** (C√°lculo de tensores e redes neurais)
* **NumPy** (Manipula√ß√£o de matrizes e amostragem)
* **Matplotlib** (Visualiza√ß√£o de dados)

## üß† Detalhes do Modelo

O modelo utiliza a t√©cnica **Skip-gram**, onde o objetivo √© prever as palavras de contexto a partir de uma √∫nica palavra central.

A probabilidade √© calculada atrav√©s da fun√ß√£o **Softmax** aplicada ao produto escalar dos vetores de embedding:

* : Vetor da palavra central.
* : Vetor da palavra de contexto.

## üìä Resultados e Visualiza√ß√£o

Ap√≥s 50.000 √©pocas de treinamento (em um corpus pequeno), o modelo √© capaz de mapear as palavras em um espa√ßo bidimensional. No notebook, voc√™ encontrar√° um gr√°fico que demonstra, por exemplo, que palavras como `dog`, `cat` e `animal` tendem a se aproximar, assim como `banana`, `apple` e `fruit`.

![Visualiza√ß√£o dos Embeddings](assets/plot.png)

## üìÇ Como Usar

1. Clone o reposit√≥rio:
```bash
git clone https://github.com/Enzoonofre/NLP/Word2Vec.git

```


2. Certifique-se de ter as depend√™ncias instaladas:
```bash
pip install torch numpy matplotlib

```


3. Abra e execute o arquivo `Word2Vec.ipynb` em um ambiente Jupyter ou Google Colab.

---

## üìö Refer√™ncias e Agradecimentos

Este projeto foi desenvolvido para fins de estudo sobre Processamento de Linguagem Natural (NLP). 

* **Aula Original:** A l√≥gica e estrutura base deste algoritmo foram inspiradas no tutorial de [Chonlakorn (p-lain)](https://www.youtube.com/watch?v=KwjrTVaQKjA), que aborda implementa√ß√µes fundamentais de NLP em PyTorch.
* **Artigo Original:** Mikolov, T., et al. (2013). [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781).
