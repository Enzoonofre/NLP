{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Introdução\n",
        "\n",
        "Este notebook apresenta um projeto de **Processamento de Linguagem Natural (NLP)** com foco na **análise de sentimentos em tweets**, utilizando um conjunto de dados rotulado da biblioteca **NLTK**. O objetivo é construir um pipeline completo de classificação de sentimentos, passando pelas etapas de limpeza dos textos, extração de características, treinamento e avaliação de um modelo de **Regressão Logística**.\n",
        "\n",
        "---\n",
        "\n",
        "## Etapas do Projeto\n",
        "\n",
        "### Parte 1 - Importação e exploração do dataset\n",
        "\n",
        "* Carregamento dos tweets com sentimentos positivos e negativos usando os recursos do NLTK.\n",
        "* Análise inicial para entender o formato e a distribuição dos dados.\n",
        "\n",
        "### Parte 2 - Pré-processamento dos textos\n",
        "\n",
        "* Limpeza dos tweets (remoção de menções, URLs, hashtags, pontuação e símbolos).\n",
        "* Tokenização, remoção de stopwords e aplicação de **stemming** para padronização das palavras.\n",
        "\n",
        "### Parte 3 - Análise de frequência\n",
        "\n",
        "* Contagem das palavras mais frequentes associadas a sentimentos positivos e negativos.\n",
        "* Criação de um dicionário de frequências para alimentar o modelo de classificação.\n",
        "\n",
        "### Parte 4 - Regressão logística\n",
        "\n",
        "* Implementação da função sigmoide, da função de custo e do algoritmo de **gradiente descendente**.\n",
        "* Definição do vetor de características para cada tweet.\n",
        "\n",
        "### Parte 5 - Treinamento do modelo\n",
        "\n",
        "* Aplicação do gradiente descendente para aprender os pesos (theta) do modelo.\n",
        "* Exibição do custo final e dos pesos ajustados após o treinamento.\n",
        "\n",
        "### Parte 6 - Testando o modelo\n",
        "\n",
        "* Criação da função para prever o sentimento de um novo tweet com base nos pesos aprendidos.\n",
        "* Utilização da função `predicao_tweet()` para estimar a probabilidade de sentimento positivo.\n",
        "\n",
        "### Parte 7 - Verificar o desempenho usando o conjunto de teste\n",
        "\n",
        "* Avaliação do modelo em dados não vistos.\n",
        "* Cálculo da **acurácia** com base nas previsões feitas no conjunto de teste.\n",
        "\n",
        "### Parte 8 - Teste seu próprio tweet\n",
        "\n",
        "* Cédula simples para o usuário inserir seu próprio tweet e obter a previsão de sentimento feita pelo modelo.\n"
      ],
      "metadata": {
        "id": "2M_iivNHM9ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importação das bibliotecas necessárias para criação do projeto"
      ],
      "metadata": {
        "id": "KUIBdHWAOWKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "QpqkM9t_Ms5P"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 - Importação e exploração do dataset"
      ],
      "metadata": {
        "id": "IRREGnYNO-yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVQxBrvdOVum",
        "outputId": "827d07d3-cef5-44fb-ac0e-2097666b5470"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "RLi5DRgHPahs"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numeros de tweets positivos: \" , len(all_positive_tweets))\n",
        "print(\"Numeros de tweets negativos: \", len(all_negative_tweets) )\n",
        "\n",
        "print(\"O tipo das variaveis 'all_positive_tweets/all_negative_tweets': \", type(all_positive_tweets))\n",
        "print('O tipo dos objetos(tweets) dentro dessas variaveis ', type(all_positive_tweets[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZIyPcbXPd9D",
        "outputId": "096236ec-7a77-4882-fe46-a0ea62371a2b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeros de tweets positivos:  5000\n",
            "Numeros de tweets negativos:  5000\n",
            "O tipo das variaveis 'all_positive_tweets/all_negative_tweets':  <class 'list'>\n",
            "O tipo dos objetos(tweets) dentro dessas variaveis  <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printando positivo em verde\n",
        "print('\\033[92m' + all_positive_tweets[np.random.randint(0,5000)])\n",
        "\n",
        "# printando negativo em vermelho\n",
        "print('\\033[91m' + all_negative_tweets[np.random.randint(0,5000)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPb88EiSQwEf",
        "outputId": "ca1facf6-cb41-4927-b7ff-c05a7687adfe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAn apartment makeover? http://t.co/ctLa1jppdb great ideas for factory living :)\n",
            "\u001b[91mdidnt took photos with you :-(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parte 2 Pré-processamento dos textos"
      ],
      "metadata": {
        "id": "6Bi6BpjXRMCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations"
      ],
      "metadata": {
        "id": "-v4vMO3DRxln"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para essa parte utilizaremos steamming, remoção de stop words e tokenização das palavras, utilizaremos um tweet aleatorio como exemplo para ilustrar cada parte do processo"
      ],
      "metadata": {
        "id": "iRKZTygPRU7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = all_positive_tweets[2277]\n",
        "print(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGXL2BjKRSG3",
        "outputId": "7cd656ac-1d92-4c3e-bc12-197615aba0c9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTCbaQRfRrRA",
        "outputId": "5d8dce7d-77a8-4af0-a25c-85d10ffba68f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def limpar_tweet(tweet):\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "  return tweet\n",
        "\n",
        "#Guardaremos essa função pois quando for preciso fazer usa-lá para todo conjunto de dados chamaremos ela novamente"
      ],
      "metadata": {
        "id": "fw22I1mpRyXw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento(tweet):\n",
        "  tweet = limpar_tweet(tweet)\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  tweet_clean = []\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and #remove stopwords\n",
        "        word not in string.punctuation): #remove pontuação\n",
        "      tweet_clean.append(word)\n",
        "  return tweet_clean\n",
        "\n"
      ],
      "metadata": {
        "id": "azUZWaMcSw0T"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweet)\n",
        "print(pre_processamento(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzo2eTZ6T7PX",
        "outputId": "6e8441c0-504d-489e-9b5f-5ad604983c81"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
            "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def steamming(tweet_clean):\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  tweets_stem = []\n",
        "  for word in tweet_clean:\n",
        "    stem_word = stemmer.stem(word) # stemming palavra\n",
        "    tweets_stem.append(stem_word)\n",
        "  return tweets_stem\n",
        "\n"
      ],
      "metadata": {
        "id": "Ipwl-grTUiW0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweet)\n",
        "print(pre_processamento(tweet))\n",
        "print(steamming(pre_processamento(tweet)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9mrSZZ-U_L8",
        "outputId": "cc647347-3310-4ffc-bce4-bfaf5acc7dba"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
            "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n",
            "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processamento_final(tweet):\n",
        "  return steamming(pre_processamento(tweet))"
      ],
      "metadata": {
        "id": "0TbjkRBiVfmu"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parte 3 - Análise de frequência"
      ],
      "metadata": {
        "id": "lOdS0WlTXGFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construcao_freqs(tweets, ys):\n",
        "    \"\"\"Construcao de frequencias.\n",
        "    Input:\n",
        "        tweets: uma lista de tweets\n",
        "        ys: um array de m x 1 com a rotulagem de cada tweet (0,1)\n",
        "    Output:\n",
        "        freqs: um dicionário mapeando (palavra,rotulo) para a frequencia\n",
        "    \"\"\"\n",
        "    # Converte o array np em lista, pois o zip precisa de um iterável.\n",
        "    # o \"squeeze\" é necessário ou a lista acaba com um elemento.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Começa com dicionário vazio e vai populando conforme vai iterando\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in pre_processamento_final(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "metadata": {
        "id": "TwmWwN3AXNC3"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parte 4 - Regressão logística"
      ],
      "metadata": {
        "id": "hYXCQnyzaOz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando regressão logística para classificação de texto.\n",
        "\n",
        "A função sigmoide é definida como:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "Ela mapeia a entrada (z) para um valor entre 0 e 1, podendo assim ser interpretada como uma probabilidade.\n",
        "\n",
        "Implementação da função sigmoide:\n",
        "\n",
        "- Você deve garantir que essa função funcione corretamente tanto se z for um valor escalar (ex: um número como 3.5) quanto se for um array (ex: uma lista ou vetor de valores).\n",
        "\n"
      ],
      "metadata": {
        "id": "_3uEy82caRnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoide(z):\n",
        "    '''\n",
        "    Entrada:\n",
        "        z: valor de entrada (pode ser um escalar ou um array)\n",
        "    Saída:\n",
        "        h: o resultado da função sigmoide aplicada a z\n",
        "    '''\n",
        "\n",
        "    # calcula a função sigmoide de z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return h\n"
      ],
      "metadata": {
        "id": "xUYSIrU2a6V3"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando a função\n",
        "if (sigmoide(0) == 0.5):\n",
        "    print('SUCESSO!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoide(4.92) == 0.9927537604041685):\n",
        "    print('CORRETO!')\n",
        "else:\n",
        "    print('Oops denovo!')\n",
        "\n",
        "print(sigmoide(np.array([0, 4.92])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD-IO6-abkEl",
        "outputId": "c10aff5f-7dd2-4423-df39-555d13dd756d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCESSO!\n",
            "CORRETO!\n",
            "[0.5        0.99275376]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressão Logística: regressão + função sigmoide\n",
        "\n",
        "A **regressão logística** começa com uma **regressão linear comum** e, em seguida, aplica uma **função sigmoide** ao resultado dessa regressão.\n",
        "\n",
        "#### Regressão Linear:\n",
        "\n",
        "$$\n",
        "z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_N x_N\n",
        "$$\n",
        "\n",
        "Observe que os valores de \\$\\theta\\$ são os **pesos**.\n",
        "Usaremos a notação \\$\\theta\\$ para representar esses pesos.\n",
        "\n",
        "#### Regressão Logística:\n",
        "\n",
        "$$\n",
        "h(z) = \\frac{1}{1+\\exp^{-z}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_N x_N\n",
        "$$\n",
        "\n",
        "Chamaremos o valor `z` de **logits**.\n",
        "\n",
        "---\n",
        "\n",
        "Se quiser que eu converta isso diretamente em uma célula Markdown pronta para o Colab, posso te passar também.\n"
      ],
      "metadata": {
        "id": "NYpgbdbHdKyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte 4.2  – Função de Custo e Gradiente\n",
        "\n",
        "A **função de custo** usada na regressão logística é a **média da perda logarítmica (log loss)** em todos os exemplos de treinamento:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1 - y^{(i)})\\log (1 - h(z(\\theta)^{(i)})) \\right] \\tag{5}\n",
        "$$\n",
        "\n",
        "Onde:\n",
        "\n",
        "* $m$ é o número de exemplos de treinamento\n",
        "* $y^{(i)}$ é o **rótulo real** do exemplo de número *i*\n",
        "* $h(z(\\theta)^{(i)})$ é a **previsão do modelo** para o exemplo *i*\n",
        "\n",
        "---\n",
        "\n",
        "#### Função de perda para um único exemplo:\n",
        "\n",
        "$$\n",
        "Loss = -1 \\times \\left[ y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1 - y^{(i)})\\log (1 - h(z(\\theta)^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Observações importantes:\n",
        "\n",
        "* Todos os valores de $h$ estão entre 0 e 1, então os logaritmos serão **negativos**. Por isso, usamos o fator **-1** para tornar a soma positiva.\n",
        "* Quando o modelo prevê 1 ($h(z(\\theta)) = 1$) e o rótulo $y$ também é 1, a perda é **zero**.\n",
        "* O mesmo acontece se o modelo prevê 0 e o rótulo real também for 0: a perda será **zero**.\n",
        "* No entanto, se o modelo prevê algo muito próximo de 1 ($h(z(\\theta)) = 0.9999$) e o rótulo real é 0, o segundo termo da perda se torna um número muito **negativo**, e multiplicando por -1 resulta em uma **perda positiva alta**.\n",
        "\n",
        "Exemplo:\n",
        "\n",
        "$$\n",
        "-1 \\times (1 - 0) \\times \\log(1 - 0.9999) \\approx 9.2\n",
        "$$\n",
        "\n",
        "👉 Quanto mais próximo de 1 for a previsão incorreta, **maior será a penalidade**."
      ],
      "metadata": {
        "id": "OKhI0YL0dezY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verifique que, quando o modelo prevê um valor próximo de 1, mas o rótulo real é 0, a perda é um valor positivo alto\n",
        "-1 * (1 - 0) * np.log(1 - 0.9999)  # perda é aproximadamente 9.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWuz_8dhdKHo",
        "outputId": "29a89349-9f58-43e6-900d-66f765f5a339"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(9.210340371976294)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Atualização dos Pesos\n",
        "\n",
        "Para atualizar o vetor de pesos $\\theta$, aplicarei o **Gradiente Descendente** para melhorar iterativamente as previsões do modelo.\n",
        "\n",
        "O gradiente da função de custo $J$ em relação a um dos pesos $\\theta_j$ é dado por:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)} - y^{(i)})x_j \\tag{5}\n",
        "$$\n",
        "\n",
        "Onde:\n",
        "\n",
        "* $i$ é o índice de cada um dos $m$ exemplos de treinamento\n",
        "* $j$ é o índice do peso $\\theta_j$, e $x_j$ é a característica (feature) associada a esse peso\n",
        "\n",
        "---\n",
        "\n",
        "Para atualizar o peso $\\theta_j$, ajustamos seu valor **subtraindo uma fração do gradiente**, determinada pela taxa de aprendizado $\\alpha$:\n",
        "\n",
        "$$\n",
        "\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta)\n",
        "$$\n",
        "\n",
        "* A **taxa de aprendizado** $\\alpha$ é um valor escolhido manualmente e **controla o tamanho de cada atualização** dos pesos."
      ],
      "metadata": {
        "id": "Rw8d87BoeU4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Entrada:\n",
        "        x: matriz de características (features), com dimensões (m, n+1)\n",
        "        y: rótulos correspondentes à matriz de entrada x, dimensões (m, 1)\n",
        "        theta: vetor de pesos com dimensão (n+1, 1)\n",
        "        alpha: taxa de aprendizado\n",
        "        num_iters: número de iterações que você deseja treinar seu modelo\n",
        "    Saída:\n",
        "        J: custo final\n",
        "        theta: vetor de pesos final\n",
        "    Dica: você pode querer imprimir o custo para garantir que ele está diminuindo.\n",
        "    '''\n",
        "    ### INÍCIO DO CÓDIGO ###\n",
        "    # obtém 'm', o número de linhas da matriz x\n",
        "    m = x.shape[0]\n",
        "\n",
        "    for i in range(0, num_iters):\n",
        "\n",
        "        # calcula z, o produto escalar de x e theta\n",
        "        z = np.dot(x, theta)\n",
        "\n",
        "        # calcula h, a função sigmoide aplicada a z\n",
        "        h = sigmoide(z)\n",
        "\n",
        "        # calcula a função de custo\n",
        "        # note que também poderíamos usar np.array.transpose() ao invés de np.array.T\n",
        "        # np.array.T apenas deixa o código mais legível :)\n",
        "        J = -1./m * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "\n",
        "        # atualiza os pesos theta\n",
        "        theta = theta - (alpha/m) * np.dot(x.T, (h - y))\n",
        "\n",
        "    ### FIM DO CÓDIGO ###\n",
        "    J = float(J)\n",
        "    return J, theta\n"
      ],
      "metadata": {
        "id": "trQRnukLfnxc"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica a função\n",
        "# Constrói um caso de teste sintético usando funções de geração de números aleatórios do numpy\n",
        "np.random.seed(1)  # Define a semente para reprodução dos resultados\n",
        "\n",
        "# A entrada X é uma matriz 10 x 3 com uns na primeira coluna para o termo de bias\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "\n",
        "# Os rótulos Y são uma matriz 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "\n",
        "# Aplica o gradiente descendente\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "\n",
        "# Imprime o custo após o treinamento\n",
        "print(f\"O custo após o treinamento é {tmp_J:.8f}.\")\n",
        "\n",
        "# Imprime o vetor de pesos resultante\n",
        "print(f\"O vetor resultante de pesos é {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzTmCJ-Lg4-H",
        "outputId": "6893aaac-5386-4dcc-988c-6fcacb704d72"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O custo após o treinamento é 0.67094970.\n",
            "O vetor resultante de pesos é [np.float64(4.1e-07), np.float64(0.00035658), np.float64(7.309e-05)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-dde2da729d09>:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  J = float(J)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Entrada:\n",
        "        tweet: uma lista de palavras (tokens) de um tweet\n",
        "        freqs: um dicionário com a contagem de vezes que (palavra, label) aparece.\n",
        "\n",
        "    Saída:\n",
        "        x: um vetor de características com dimensão (1,3)\n",
        "           [1, contagem_palavras_positivas, contagem_palavras_negativas]\n",
        "    '''\n",
        "\n",
        "    # processa o tweet (remove stopwords, faz stemming, etc.)\n",
        "    word_l = pre_processamento_final(tweet)\n",
        "\n",
        "    # cria um vetor de características com 3 posições\n",
        "    x = np.zeros((1, 3))\n",
        "\n",
        "    # o primeiro valor é sempre 1 (termo de viés/bias)\n",
        "    x[0,0] = 1\n",
        "\n",
        "    # percorre todas as palavras processadas do tweet\n",
        "    for word in word_l:\n",
        "\n",
        "        # se a palavra aparecer com o rótulo positivo (1.0) no dicionário, soma a frequência\n",
        "        if (word, 1.0) in freqs:\n",
        "            x[0,1] += freqs[(word, 1.0)]\n",
        "\n",
        "        # se a palavra aparecer com o rótulo negativo (0.0), soma a frequência também\n",
        "        if (word, 0.0) in freqs:\n",
        "            x[0,2] += freqs[(word, 0.0)]\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "EzHUL5BxkeZZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 5 - Treinamento do modelo"
      ],
      "metadata": {
        "id": "qlkRnYrXevfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#separando em teste e treino do modelo\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "iGytyY6t1vU_"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combina rotulos positivos e negativos\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n"
      ],
      "metadata": {
        "id": "B8J0JHGV18Cg"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime o shape\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XX9cKAW3Air",
        "outputId": "8c353389-2ae5-4120-dac2-cde21f8b3a9d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = construcao_freqs(train_x, train_y)\n",
        "\n",
        "print('type(freqs) = '+str(type(freqs)))\n",
        "print('len(freqs)= '+str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKis5LAb3maz",
        "outputId": "5445e3f6-2926-4311-dbfd-d2ce8128f98f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs)= 11306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print('Exemplo de um tweet positivo: \\n', train_x[0])\n",
        "print('\\nExemplo de um tweet após ser processado: \\n', pre_processamento_final(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsedLQ9D4OOR",
        "outputId": "f2b417e7-9e32-4e42-b368-f13ae759843e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemplo de um tweet positivo: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "Exemplo de um tweet após ser processado: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(train_x),3))\n",
        "for i in range(len(train_x)):\n",
        "  X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "Y = train_y\n",
        "\n",
        "J , theta = gradientDescent(X, Y, np.zeros((3,1)), 1e-9, 1500)\n",
        "\n",
        "print(f\"O custo após o treinamento é {J:.8f}.\")\n",
        "print(f\"O vetor resultante de pesos é {[round(t, 8) for t in np.squeeze(theta)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZlUf1i-4l-0",
        "outputId": "63c231ca-b608-40ed-9e74-5798b3b4dc52"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O custo após o treinamento é 0.24217224.\n",
            "O vetor resultante de pesos é [np.float64(7e-08), np.float64(0.00052352), np.float64(-0.00055579)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-dde2da729d09>:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  J = float(J)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 6 - Testando modelo"
      ],
      "metadata": {
        "id": "MKnp9wVF5a41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4 (IDENTIFICADOR ÚNICO DA CÉLULA, NÃO EDITAR)\n",
        "def predicao_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Entrada:\n",
        "        tweet: uma string (o texto do tweet)\n",
        "        freqs: um dicionário com a frequência de cada tupla (palavra, rótulo)\n",
        "        theta: vetor de pesos de dimensão (3,1)\n",
        "\n",
        "    Saída:\n",
        "        y_pred: a probabilidade do tweet ser positivo (ou negativo)\n",
        "    '''\n",
        "    ### INÍCIO DO CÓDIGO ###\n",
        "\n",
        "    # extrai as características do tweet e armazena em x\n",
        "    x = extract_features(tweet, freqs)\n",
        "\n",
        "    # faz a previsão usando x e theta (produto escalar e função sigmoide)\n",
        "    y_pred = sigmoide(np.dot(x, theta))\n",
        "\n",
        "    ### FIM DO CÓDIGO ###\n",
        "\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "Tbrrpr__5aIu"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo para testar a predicao\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predicao_tweet(tweet, freqs, theta)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCDypymm59zD",
        "outputId": "cc18ccf3-a587-4145-efb2-53ec36a79d28"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 0.518562\n",
            "I am bad -> 0.494329\n",
            "this movie should have been great. -> 0.515312\n",
            "great -> 0.515449\n",
            "great great -> 0.530868\n",
            "great great great -> 0.546228\n",
            "great great great great -> 0.561501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-5ac626e7c0f5>:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print( '%s -> %f' % (tweet, predicao_tweet(tweet, freqs, theta)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meu_tweet = 'I am learning :)'\n",
        "predicao_tweet(meu_tweet, freqs, theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggyy0S7b6I-c",
        "outputId": "3c929579-02a5-46ef-95dc-c495acbdec94"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8162007]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##  Parte 7 - Verificar o desempenho usando o conjunto de teste\n",
        "\n",
        "Depois de treinar o seu modelo com o conjunto de treinamento, é hora de verificar **como ele se comporta com dados reais e inéditos**, testando-o com o conjunto de teste.\n",
        "\n",
        "---\n",
        "\n",
        "###  Instruções: Implemente a função `test_logistic_regression`\n",
        "\n",
        "O objetivo é **calcular a acurácia** (percentual de acertos) do seu modelo de regressão logística usando os dados de teste e os pesos `theta` que você treinou.\n",
        "\n",
        "---\n",
        "\n",
        "###  Etapas para implementar:\n",
        "\n",
        "1. Para cada tweet do conjunto de teste, use a função `predict_tweet()` para fazer a **previsão de probabilidade**.\n",
        "\n",
        "2. Se a **probabilidade prevista for maior que 0.5**, você considera que o modelo classificou o tweet como **positivo** (ou seja, `y_hat = 1`).\n",
        "   Caso contrário, a classificação é **negativa** (`y_hat = 0`).\n",
        "\n",
        "3. Compare a previsão `y_hat` com o valor real do rótulo (`test_y`).\n",
        "\n",
        "4. Conte **quantas vezes a previsão foi correta**, ou seja, `y_hat == test_y`.\n",
        "\n",
        "5. Calcule a acurácia:\n",
        "\n",
        "   $$\n",
        "   \\text{acurácia} = \\frac{\\text{número de acertos}}{m}\n",
        "   $$\n",
        "\n",
        "   onde `m` é o número total de exemplos no conjunto de teste.\n"
      ],
      "metadata": {
        "id": "GDGJ_9IA6k3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testar_regressao_logistica(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Entrada:\n",
        "        test_x: uma lista de tweets\n",
        "        test_y: vetor (m, 1) com os rótulos reais correspondentes a cada tweet\n",
        "        freqs: um dicionário com a frequência de cada par (palavra, rótulo)\n",
        "        theta: vetor de pesos com dimensão (3, 1)\n",
        "    Saída:\n",
        "        acuracia: (número de tweets classificados corretamente) / (número total de tweets)\n",
        "    \"\"\"\n",
        "\n",
        "    # lista para armazenar as previsões\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # obter a previsão do rótulo para o tweet\n",
        "        y_pred = predicao_tweet(tweet, freqs, theta)\n",
        "\n",
        "        if y_pred > 0.5:\n",
        "            # adicionar 1.0 à lista se for positivo\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # adicionar 0 se for negativo\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # y_hat é uma lista e test_y é um array (m,1)\n",
        "    # converter ambos para arrays unidimensionais para comparar com o operador '=='\n",
        "    acuracia = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n",
        "\n",
        "    return acuracia\n"
      ],
      "metadata": {
        "id": "sf5x8t-86n75"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = testar_regressao_logistica(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5hCpDPp7Nc4",
        "outputId": "9c2ff066-3e37-4a8d-dfdd-b762a9c066e6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parte 8 - Teste seu próprio tweet"
      ],
      "metadata": {
        "id": "fKD_O7Q77Ze4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pode se sentir livre para mudar o tweet abaixo\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(pre_processamento_final(my_tweet))\n",
        "y_hat = predicao_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else:\n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQiXwX8U7hsy",
        "outputId": "015ef35f-def7-425e-c6b8-31bd47f84360"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.48136088]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    }
  ]
}